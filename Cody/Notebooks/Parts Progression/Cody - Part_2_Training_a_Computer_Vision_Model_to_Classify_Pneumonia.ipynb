{"cells":[{"cell_type":"markdown","metadata":{"id":"uMfGLGNLnXk7"},"source":["## AIMI High School Internship 2024\n","### Notebook 2: Training a Computer Vision Model to Classify Pneumonia\n","\n","**The Problem**: Given a chest X-ray, our goal in this project is to classify the image into one of four classes: **pneumonia, pneumothorax, pleural effusion**, and **normal**.  \n","\n","**Your Second Task**: You should now have a training dataset consisting of (a) chest X-rays and (b) labels extracted from radiologist reports processed using NLP or a similar technique. Now, your goal is to train a computer vision model to classify the images. You have **two options** for this task, and you may attempt one or both of these:\n","- *Standard Classification* : Train a model to predict which class of pneumonia a chest x-ray belongs to using image-only derived features.\n","- *Classification w/ Metadata (stretch)*: Train a model that predicts which class of pneumonia a chest x-ray belongs to using image and additional patient metadata-derived features.\n","\n","In this notebook, we provide some simple starter code to get you started on training a computer vision model. You are not required to use this template - feel free to modify as you see fit.\n","\n","**Submitting Your Model**: We have created a leaderboard where you can submit your model and view results on the held-out test set. We provide instructions below for submitting your model to the leaderboard. **Please follow these directions carefully**.\n","\n","We will evaluate your results on the held-out test set with the following evaluation metrics:\n","- **Accuracy**: the ratio of correctly predicted observations to the total observations. It tells us the proportion of true results (both true positives and true negatives) among the total number of cases examined. While straightforward, accuracy can be misleading in the context of imbalanced datasets where the number of observations in different classes varies significantly.\n","- **AUROC (Area Under the Receiver Operating Characteristic curve)**: a performance measurement for classification problems at various threshold settings. It tells us how well a model is capable of distinguishing between classes. The higher the AUROC, the better the model is at predicting 0s as 0s and 1s as 1s. An AUROC of 0.5 suggests no discriminative ability (equivalent to random guessing), while an AUROC of 1.0 indicates perfect discrimination.\n","- **Precision**: the ratio of correctly predicted positive observations to the total predicted positives. It is a measure of a classifier's exactness. High precision indicates a low false positive rate. It's particularly useful when the costs of False Positives are high.\n","- **Recall**: (also known as sensitivity) the ratio of correctly predicted positive observations to the all observations in actual class - yes. It is a measure of a classifier's completeness. High recall indicates that the class is correctly recognized (a low number of False Negatives).\n","- **F1**: the harmonic mean of precision and recall. It's a way to combine both precision and recall into a single measure that captures both properties. This score can be particularly useful if you need to balance precision and recall, which is often the case in uneven class distribution scenarios. The F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0."]},{"cell_type":"markdown","metadata":{"id":"RJ1rTMrgpoil"},"source":["## Load Data\n","Before you begin, make sure to go to `Runtime` > `Change Runtime Type` and select a T4 GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24591,"status":"ok","timestamp":1719449506501,"user":{"displayName":"Cody Chen","userId":"04636862374912145551"},"user_tz":420},"id":"ytMQzLJindpR","outputId":"d7dab2d9-5385-4949-d713-32302f2fa2cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySb9AsmBp-Gz"},"outputs":[],"source":["import os\n","os.chdir(r'/content/drive/MyDrive/Cody - AIMI 2024/2024 AIMI Summer Internship - Intern Materials/Datasets')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bqeesviqp_hN"},"outputs":[],"source":["!unzip -qq student_data_split.zip -d /content/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c56zZFdKy-AP"},"outputs":[],"source":["# Switch back to /content/student_data_split folder to work with downloaded datasets\n","os.chdir(r'/content/student_data_split')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1719449607085,"user":{"displayName":"Cody Chen","userId":"04636862374912145551"},"user_tz":420},"id":"6CE488t1y-kM","outputId":"a25f17b6-0217-48af-d5e9-b04ff2cd4d03"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reports.json  student_test  student_train\n"]}],"source":["# Confirm we can now see the student_test and student_train folders + Reports.json\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"q5etX4eYtu_s"},"source":["## Import Libraries\n","We are leveraging the PyTorch framework to train our models. For more information and tutorials on PyTorch, see this link: https://pytorch.org/tutorials/beginner/basics/intro.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkVpOveTbI7k"},"outputs":[],"source":["%%capture\n","%pip install \"comet_ml>=3.38.0\" torch torchvision tqdm\n","from comet_ml import Experiment\n","from comet_ml.integration.pytorch import watch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BzhTFDi7tuPK"},"outputs":[],"source":["# Some libraries that you may find useful are included here.\n","# To import a library that isn't provided with Colab, use the following command: !pip install <package_name>\n","import torch\n","import pandas as pd\n","from PIL import Image\n","import numpy as np\n","from tqdm import tqdm\n","from torchvision.transforms import v2\n","from torch import nn\n","from torchvision import models\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":840,"status":"ok","timestamp":1719449692186,"user":{"displayName":"Cody Chen","userId":"04636862374912145551"},"user_tz":420},"id":"LP-26TkfzJTE","outputId":"6ecc3aa8-e6b3-4e12-9296-8ba2646b9ed2"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"dataframe\",\n  \"rows\": 16772,\n  \"fields\": [\n    {\n      \"column\": \"Patient ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10884,\n        \"samples\": [\n          \"patient26839\",\n          \"patient07318\",\n          \"patient29331\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Study ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12716,\n        \"samples\": [\n          \"student_train/patient07204/study1\",\n          \"student_train/patient06204/study8\",\n          \"student_train/patient04774/study3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Image Path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16772,\n        \"samples\": [\n          \"student_train/patient57836/study1/view1_frontal.jpg\",\n          \"student_train/patient03577/study3/view2_lateral.jpg\",\n          \"student_train/patient00689/study10/view1_frontal.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"pneumothorax\",\n          \"pneumonia, pneumothorax\",\n          \"normal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Encoded Labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"dataframe"},"text/html":["\n","  <div id=\"df-2786aa31-d1e3-4c16-94ae-b231f41536d4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Patient ID</th>\n","      <th>Study ID</th>\n","      <th>Image Path</th>\n","      <th>Label</th>\n","      <th>Encoded Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>patient39668</td>\n","      <td>student_train/patient39668/study2</td>\n","      <td>student_train/patient39668/study2/view1_fronta...</td>\n","      <td>normal</td>\n","      <td>[0.0, 0.0, 0.0, 1.0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>patient17014</td>\n","      <td>student_train/patient17014/study2</td>\n","      <td>student_train/patient17014/study2/view1_fronta...</td>\n","      <td>pneumothorax</td>\n","      <td>[0.0, 1.0, 0.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>patient11443</td>\n","      <td>student_train/patient11443/study1</td>\n","      <td>student_train/patient11443/study1/view1_fronta...</td>\n","      <td>pneumothorax</td>\n","      <td>[0.0, 1.0, 0.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>patient29294</td>\n","      <td>student_train/patient29294/study1</td>\n","      <td>student_train/patient29294/study1/view1_fronta...</td>\n","      <td>pneumothorax, pleural effusion</td>\n","      <td>[0.0, 1.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>patient34615</td>\n","      <td>student_train/patient34615/study71</td>\n","      <td>student_train/patient34615/study71/view1_front...</td>\n","      <td>pleural effusion</td>\n","      <td>[0.0, 0.0, 1.0, 0.0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2786aa31-d1e3-4c16-94ae-b231f41536d4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2786aa31-d1e3-4c16-94ae-b231f41536d4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2786aa31-d1e3-4c16-94ae-b231f41536d4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-bd221df8-c81f-4fec-9c7a-8bcaea45723f\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd221df8-c81f-4fec-9c7a-8bcaea45723f')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-bd221df8-c81f-4fec-9c7a-8bcaea45723f button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["     Patient ID                            Study ID  \\\n","0  patient39668   student_train/patient39668/study2   \n","1  patient17014   student_train/patient17014/study2   \n","2  patient11443   student_train/patient11443/study1   \n","3  patient29294   student_train/patient29294/study1   \n","4  patient34615  student_train/patient34615/study71   \n","\n","                                          Image Path  \\\n","0  student_train/patient39668/study2/view1_fronta...   \n","1  student_train/patient17014/study2/view1_fronta...   \n","2  student_train/patient11443/study1/view1_fronta...   \n","3  student_train/patient29294/study1/view1_fronta...   \n","4  student_train/patient34615/study71/view1_front...   \n","\n","                            Label        Encoded Labels  \n","0                          normal  [0.0, 0.0, 0.0, 1.0]  \n","1                    pneumothorax  [0.0, 1.0, 0.0, 0.0]  \n","2                    pneumothorax  [0.0, 1.0, 0.0, 0.0]  \n","3  pneumothorax, pleural effusion  [0.0, 1.0, 1.0, 0.0]  \n","4                pleural effusion  [0.0, 0.0, 1.0, 0.0]  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Load your image paths and extracted labels from your saved file\n","#dataset = pd.read_pickle(\"/content/drive/MyDrive/Cody - AIMI 2024/conditionsDf.pkl\")\n","dataframe = pd.read_pickle(\"/content/drive/MyDrive/Cody - AIMI 2024/train_data.pkl\")\n","\n","\n","# Display the first few rows of the DataFrame to confirm it's loaded correctly\n","dataframe.head()"]},{"cell_type":"markdown","metadata":{"id":"QY7yvIM0yl4M"},"source":["## Create Dataloaders\n","We will implement a custom Dataset class to load in data. A custom Dataset class must have three methods: `__init__`, which sets up any class variables, `__len__`, which defines the total number of images, and `__getitem__`, which returns a single image and its paired label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwH5586UqAnY"},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class ChestXRayDataset(Dataset):\n","    def __init__(self, dataframe, transforms):\n","        #super(ChestXRayDataset, self).__init__(**kwargs)\n","\n","        self.dataframe = dataframe\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","\n","    def __getitem__(self, idx):\n","        out_dict = {\"idx\": torch.tensor(idx),}\n","\n","        image_path = self.dataframe.loc[idx,'Image Path']\n","        labels = self.dataframe.loc[idx,'Encoded Labels']\n","\n","        image = Image.open(image_path).convert(\"RGB\")\n","        #image = torch.tensor(image, dtype=torch.float32)\n","        if(self.transforms is not None):\n","            image = self.transforms(image)\n","\n","        out_dict[\"img\"] = image\n","        out_dict[\"label\"] = torch.tensor(labels, dtype=torch.float32)\n","\n","        #return out_dict\n","        return out_dict[\"img\"], out_dict[\"label\"]\n","        #image, target"]},{"cell_type":"markdown","metadata":{"id":"2oGRC8Mk0ytJ"},"source":["## Define Training Components\n","Here, define any necessary components that you need to train your model, such as the model architecture, the loss function, and the optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YKU5fxjbdYsb"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bb5XsojSuhRn"},"outputs":[],"source":["class Resnext50(nn.Module):\n","    def __init__(self, n_classes):\n","        super().__init__()\n","        resnet = models.resnext50_32x4d(pretrained=True)\n","        resnet.fc = nn.Sequential(\n","            nn.Dropout(p=0.2),\n","            nn.Linear(in_features=resnet.fc.in_features, out_features=n_classes)\n","        )\n","        self.base_model = resnet\n","        self.sigm = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        return self.sigm(self.base_model(x))"]},{"cell_type":"markdown","metadata":{"id":"64NGouxxc7oY"},"source":["Weighted classes for loss function to deal with imbalanced classes (from Harjyot)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":467,"status":"ok","timestamp":1719449692651,"user":{"displayName":"Cody Chen","userId":"04636862374912145551"},"user_tz":420},"id":"naWj_93jc_-G","outputId":"37d54ca3-85f1-480d-dd84-895d1de98298"},"outputs":[{"name":"stdout","output_type":"stream","text":["Class weights: [0.58977703 0.22927263 0.16137105 0.01957929]\n"]}],"source":["#Class frequencies\n","#class_frequencies = np.array([449, 1155, 1641, 13525])\n","\n","#Inverse of class frequencies\n","#class_weights = 1.0 / class_frequencies\n","#Normalize class weights\n","#class_weights /= class_weights.sum()\n","\n","#print(\"Class weights:\", class_weights)\n","\n","#class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1m0OzkoKY23b"},"outputs":[],"source":["#hyperparameters\n","\n","batch_size = 64\n","k_folds = 5\n","num_epochs_per_k = 5\n","learning_rate = 1e-4\n","\n","current_epoch = 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2257,"status":"ok","timestamp":1719449694906,"user":{"displayName":"Cody Chen","userId":"04636862374912145551"},"user_tz":420},"id":"-HeB-_-k0x_S","outputId":"4ac3bf09-16b6-4179-b85a-29e2c473784f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n","100%|██████████| 95.8M/95.8M [00:01<00:00, 86.8MB/s]\n"]}],"source":["#loss_fn = torch.nn.BCEWithLogitsLoss(weight = class_weights_tensor)\n","loss_fn = torch.nn.BCEWithLogitsLoss()\n","\n","transforms = v2.Compose([\n","    v2.ToImage(),\n","    v2.Resize(size=(224, 224), antialias=True),\n","    #v2.RandomHorizontalFlip(p=0.5),\n","    v2.ToDtype(torch.float32, scale=True),\n","    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","train_dataset = ChestXRayDataset(dataframe, transforms)\n","#dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=\"\"\"Customize batch size\"\"\", shuffle=True, drop_last=True)\n","\n","num_classes = 4\n","label_space = ['pneumonia', 'pneumothorax', 'pleural effusion', 'normal']\n","\n","model = Resnext50(num_classes)\n","model.to(device)\n","opt = torch.optim.AdamW(model.parameters(), lr=learning_rate) # AdamW is a commonly-used optimizer. Feel free to modify.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9BRmuODcYUFr"},"outputs":[],"source":["experiment = Experiment(\n","  api_key=\"REDACTED\",\n","  project_name=\"aimi2024-resnext50\",\n","  workspace=\"summit\"\n",")\n","watch(model)\n"]},{"cell_type":"markdown","metadata":{"id":"1vN3XUsYmlUo"},"source":["## Visualizations\n","- Create some visualizations to highlight model performance e.g. `multilabel_confusion_matrix`, plot of train vs val loss history, plot of train vs val accuracy history."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1719447020543,"user":{"displayName":"Cody Chen","userId":"04636862374912145551"},"user_tz":420},"id":"j_n2oGFcm4jW","outputId":"32871331-d3cf-4fae-f86c-d7defa409aac"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["current_epoch"]},{"cell_type":"markdown","metadata":{"id":"TXvOaNPB1OGH"},"source":["## Training Code\n","We provide starter code below that implements a simple training loop in PyTorch. Feel free to modify as you see fit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DyeNBV5c7f_T"},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, roc_auc_score\n","\n","'''\n","def calculate_metrics_original(pred, target, threshold=0.5):\n","    pred = np.array(pred > threshold, dtype=float)\n","    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n","            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n","            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n","            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n","            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n","            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'), #stick with macro\n","            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n","            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n","            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples'),\n","            }\n","'''\n","def calculate_metrics(pred, target, threshold=0.5):\n","    thresholded_preds = np.empty_like(pred)\n","    thresholded_preds[:] = pred\n","    thresholded_preds = np.array(thresholded_preds > threshold, dtype=float)\n","\n","    f1 = f1_score(y_true=target, y_pred=thresholded_preds, average=None)\n","    f1_macro = f1_score(y_true=target, y_pred=thresholded_preds, average='macro')\n","\n","    auc = roc_auc_score(y_true=target, y_score=pred, average=None)\n","    auc_macro = roc_auc_score(y_true=target, y_score=pred, average='macro')\n","\n","    return {'f1': f1, 'f1_macro': f1_macro, 'auc': auc, 'auc_macro': auc_macro}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pWnzKime0exc"},"outputs":[],"source":["def train(model, loss_fn, train_loader, opt, max_epoch, current_epoch):\n","\n","    best_val_loss = np.inf\n","    best_val_metrics = []\n","    test_freq = 1\n","\n","    for epoch in range(0, max_epoch):\n","        current_epoch += 1\n","\n","        print(f\"Training epoch {current_epoch}\")\n","        current_loss = 0.0\n","\n","        model.train()\n","\n","        for index, (inputs, targets) in enumerate(tqdm(train_loader)):\n","        #for index, data in tqdm(enumerate(train_loader, 0)):\n","        #  inputs, targets = data\n","\n","          inputs, targets = inputs.to(device), targets.to(device)\n","          opt.zero_grad()\n","\n","          output = model(inputs)\n","          loss = loss_fn(output, targets)\n","\n","          loss.backward()\n","          opt.step()\n","\n","    return current_epoch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWl9WNpX454H"},"outputs":[],"source":["#https://stackoverflow.com/questions/42703500/how-do-i-save-a-trained-model-in-pytorch\n","#https://pytorch.org/tutorials/beginner/saving_loading_models.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"RXXz9xwndw3d","outputId":"3c9e42dc-527c-44bc-beea-de134395fe2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fold 1\n","-------\n","Training epoch 6\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:35<00:00,  1.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 7\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:26<00:00,  1.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 8\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:28<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 9\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:27<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:32<00:00,  1.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating fold...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 53/53 [00:48<00:00,  1.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["----------FOLD 1 SUMMARY---------\n","Macro F1 Score: 0.3260035636263955   Class Breakdown: [0.         0.80198265 0.5020316  0.        ]\n","Macro AUROC: 0.6802616357161302   Class Breakdown: [0.60438886 0.82289145 0.67251121 0.62125502]\n","Overall accuracy for fold 1: 79.40%\n","Accuracy for class 0: 88.76%\n","Accuracy for class 1: 76.18%\n","Accuracy for class 2: 67.12%\n","Accuracy for class 3: 85.51%\n","--------------------------------\n","Fold 2\n","-------\n","Training epoch 11\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:28<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 12\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:28<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 13\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:27<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 14\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:26<00:00,  1.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 15\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:32<00:00,  1.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating fold...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 53/53 [00:47<00:00,  1.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["----------FOLD 2 SUMMARY---------\n","Macro F1 Score: 0.3895938277623249   Class Breakdown: [0.         0.89664083 0.66173448 0.        ]\n","Macro AUROC: 0.6792755950301451   Class Breakdown: [0.47958109 0.93619114 0.78265796 0.51867218]\n","Overall accuracy for fold 2: 84.71%\n","Accuracy for class 0: 88.52%\n","Accuracy for class 1: 88.08%\n","Accuracy for class 2: 75.47%\n","Accuracy for class 3: 86.77%\n","--------------------------------\n","Fold 3\n","-------\n","Training epoch 16\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:27<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 17\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:27<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 18\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:28<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 19\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:28<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 20\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:32<00:00,  1.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating fold...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 53/53 [00:50<00:00,  1.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["----------FOLD 3 SUMMARY---------\n","Macro F1 Score: 0.4201180932786357   Class Breakdown: [0.         0.9395441  0.74092827 0.        ]\n","Macro AUROC: 0.7469879003829779   Class Breakdown: [0.56227027 0.96638799 0.83935929 0.61993405]\n","Overall accuracy for fold 3: 87.31%\n","Accuracy for class 0: 88.55%\n","Accuracy for class 1: 92.73%\n","Accuracy for class 2: 81.69%\n","Accuracy for class 3: 86.29%\n","--------------------------------\n","Fold 4\n","-------\n","Training epoch 21\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:30<00:00,  1.00s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 22\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:26<00:00,  1.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 23\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:25<00:00,  1.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 24\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:31<00:00,  1.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 25\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:25<00:00,  1.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating fold...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 53/53 [00:46<00:00,  1.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["----------FOLD 4 SUMMARY---------\n","Macro F1 Score: 0.43077266777775675   Class Breakdown: [0.         0.95346485 0.76962583 0.        ]\n","Macro AUROC: 0.7675316139631279   Class Breakdown: [0.5769287  0.97236671 0.86226553 0.65856552]\n","Overall accuracy for fold 4: 87.35%\n","Accuracy for class 0: 88.52%\n","Accuracy for class 1: 94.51%\n","Accuracy for class 2: 81.28%\n","Accuracy for class 3: 85.09%\n","--------------------------------\n","Fold 5\n","-------\n","Training epoch 26\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:26<00:00,  1.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 27\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:26<00:00,  1.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 28\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:25<00:00,  1.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 29\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:28<00:00,  1.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training epoch 30\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 210/210 [03:26<00:00,  1.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating fold...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 53/53 [00:47<00:00,  1.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["----------FOLD 5 SUMMARY---------\n","Macro F1 Score: 0.44865725948451385   Class Breakdown: [0.         0.96333501 0.83129403 0.        ]\n","Macro AUROC: 0.7847462154058129   Class Breakdown: [0.57937485 0.98213802 0.90244996 0.67502203]\n","Overall accuracy for fold 5: 89.10%\n","Accuracy for class 0: 87.87%\n","Accuracy for class 1: 95.65%\n","Accuracy for class 2: 86.43%\n","Accuracy for class 3: 86.46%\n","--------------------------------\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nprint(f\\'K-Fold cross validation results for  {k_folds} folds\\')\\nprint(\\'----------------------------------\\')\\nsums = {\"total\":0.0, 0:0.0, 1:0.0, 2:0.0, 3:0.0}\\nfor fold_idx, value in fold_results.items():\\n  for key, saved_accuracy in value.items():\\n    sums[key] += saved_accuracy\\n\\nprint(f\\'Average overall accuracy: {sums[\"total\"]/len(fold_results.items())} %\\')\\nfor key in sums.keys():\\n  if key != \"total\":\\n    print(f\\'Average accuracy for class {key} ({label_space[key]}): {sums[key]/len(fold_results.items())} %\\')\\n'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import KFold\n","\n","kf = KFold(n_splits=k_folds, shuffle=True)\n","fold_results = {}\n","threshold = 0.5\n","\n","best_macro_f1 = 0.0 #higher f1 score is better, 1 is best\n","\n","for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n","    print(f\"Fold {fold + 1}\")\n","    print(\"-------\")\n","\n","    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n","    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,\n","        sampler=train_subsampler,\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,\n","        sampler=val_subsampler,\n","    )\n","\n","    current_epoch = train(model, loss_fn, train_loader, opt, max_epoch=num_epochs_per_k, current_epoch=current_epoch)\n","\n","\n","    # Evaluate using  split\n","    print(\"Evaluating fold...\")\n","\n","    save_path = f'/content/drive/MyDrive/Cody - AIMI 2024/Trains/model-fold-{fold+1}.pth'\n","    torch.save(model.state_dict(), save_path)  # Save model weights for inference\n","    model.eval()\n","\n","    correct, total = {\"total\":0, 0:0, 1:0, 2:0, 3:0},{\"total\":0, 0:0, 1:0, 2:0, 3:0}\n","    with torch.no_grad():\n","        total_results = []\n","        total_targets = []\n","\n","        for index, (data, target) in enumerate(tqdm(val_loader)):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","\n","            total_results.extend(output.cpu().numpy())\n","            total_targets.extend(target.cpu().numpy())\n","\n","\n","            predicted = (output > threshold).int()  # Get a binary mask for predicted classes\n","            confusion_matrices = multilabel_confusion_matrix(np.array(predicted.cpu().numpy()), np.array(target.cpu().numpy()))\n","            for i, matrix in enumerate(confusion_matrices):\n","                tn, fp, fn, tp = matrix.ravel()\n","                accuracy = (tp + tn) / (tp + tn + fp + fn)\n","                correct[i] += tp + tn\n","                total[i] += tp + tn + fp + fn\n","                correct[\"total\"] += tp + tn\n","                total[\"total\"] += tp + tn + fp + fn\n","                #print(f\"Accuracy for Class {i}: {accuracy * 100:.2f}%\")\n","\n","        #Fold metrics\n","        print(f'----------FOLD {fold+1} SUMMARY---------')\n","        metrics = calculate_metrics(np.array(total_results), np.array(total_targets))\n","        print(f'Macro F1 Score: {metrics[\"f1_macro\"]}   Class Breakdown: {metrics[\"f1\"]}')\n","        print(f'Macro AUROC: {metrics[\"auc_macro\"]}   Class Breakdown: {metrics[\"auc\"]}')\n","\n","        print(f'Overall accuracy for fold {fold+1}: {(correct[\"total\"] / total[\"total\"]) * 100:.2f}%')\n","        fold_results[fold+1] = {}\n","        fold_results[fold+1][\"total\"] = 100.0 * (correct[\"total\"] / total[\"total\"])\n","        for key in correct.keys():\n","            if key != \"total\":\n","                print(f'Accuracy for class {key}: {(correct[key] / total[key]) * 100:.2f}%')\n","                fold_results[fold+1][key] = 100.0 * (correct[key] / total[key])\n","        print('--------------------------------')\n","\n","        #Save best checkpoint based on F1 Score\n","        #F1 Score preferred over AUROC b/c of data imbalance - https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc\n","        if(metrics[\"f1_macro\"] > best_macro_f1):\n","            best_macro_f1 = metrics[\"f1_macro\"]\n","            state = {\n","                'epoch': current_epoch,\n","                'state_dict': model.state_dict(),\n","                'optimizer': opt.state_dict(),\n","            }\n","            save_path = f'/content/drive/MyDrive/Cody - AIMI 2024/Trains/best.ckpt'\n","            torch.save(state, save_path)\n","\n","\n","'''\n","print(f'K-Fold cross validation results for  {k_folds} folds')\n","print('----------------------------------')\n","sums = {\"total\":0.0, 0:0.0, 1:0.0, 2:0.0, 3:0.0}\n","for fold_idx, value in fold_results.items():\n","  for key, saved_accuracy in value.items():\n","    sums[key] += saved_accuracy\n","\n","print(f'Average overall accuracy: {sums[\"total\"]/len(fold_results.items())} %')\n","for key in sums.keys():\n","  if key != \"total\":\n","    print(f'Average accuracy for class {key} ({label_space[key]}): {sums[key]/len(fold_results.items())} %')\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aDAMyAzNMNB3"},"outputs":[],"source":["#Save last ckpt\n","state = {\n","    'epoch': current_epoch,\n","    'state_dict': model.state_dict(),\n","    'optimizer': opt.state_dict(),\n","}\n","save_path = f'/content/drive/MyDrive/Cody - AIMI 2024/Trains/last.ckpt'\n","torch.save(state, save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":330,"status":"ok","timestamp":1719375222369,"user":{"displayName":"Cody Chen","userId":"04636862374912145551"},"user_tz":420},"id":"VLrQOgizEvME","outputId":"20be2127-21b7-49f3-e4a9-9188e03048b5"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n# Assuming \\'output\\' contains logits\\nthreshold = 0.5  # Adjust this threshold as needed\\npredicted = (output > threshold).int()  # Get a binary mask for predicted classes\\n\\nconfusion_matrices = multilabel_confusion_matrix(np.array(predicted.cpu().numpy()), np.array(target.cpu().numpy()))\\n\\n# Accessing individual confusion matrices\\nconfusion_matrix_class0 = confusion_matrices[0]\\nconfusion_matrix_class1 = confusion_matrices[1]\\nconfusion_matrix_class2 = confusion_matrices[2]\\nconfusion_matrix_class3 = confusion_matrices[3]\\n\\nprint(\"Confusion Matrix for Class 0:\\n\", confusion_matrix_class0)\\nprint(\"Confusion Matrix for Class 1:\\n\", confusion_matrix_class1)\\nprint(\"Confusion Matrix for Class 2:\\n\", confusion_matrix_class2)\\nprint(\"Confusion Matrix for Class 3:\\n\", confusion_matrix_class3)\\n\\nfor i, matrix in enumerate(confusion_matrices):\\n    tn, fp, fn, tp = matrix.ravel()\\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\\n    print(f\"Accuracy for Class {i}: {accuracy * 100:.2f}%\")\\n    print(\"-----------------------------------\")\\n'"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","# Assuming 'output' contains logits\n","threshold = 0.5  # Adjust this threshold as needed\n","predicted = (output > threshold).int()  # Get a binary mask for predicted classes\n","\n","confusion_matrices = multilabel_confusion_matrix(np.array(predicted.cpu().numpy()), np.array(target.cpu().numpy()))\n","\n","# Accessing individual confusion matrices\n","confusion_matrix_class0 = confusion_matrices[0]\n","confusion_matrix_class1 = confusion_matrices[1]\n","confusion_matrix_class2 = confusion_matrices[2]\n","confusion_matrix_class3 = confusion_matrices[3]\n","\n","print(\"Confusion Matrix for Class 0:\\n\", confusion_matrix_class0)\n","print(\"Confusion Matrix for Class 1:\\n\", confusion_matrix_class1)\n","print(\"Confusion Matrix for Class 2:\\n\", confusion_matrix_class2)\n","print(\"Confusion Matrix for Class 3:\\n\", confusion_matrix_class3)\n","\n","for i, matrix in enumerate(confusion_matrices):\n","    tn, fp, fn, tp = matrix.ravel()\n","    accuracy = (tp + tn) / (tp + tn + fp + fn)\n","    print(f\"Accuracy for Class {i}: {accuracy * 100:.2f}%\")\n","    print(\"-----------------------------------\")\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozAjtyygbQ83"},"outputs":[],"source":["experiment.end()"]},{"cell_type":"markdown","metadata":{"id":"T3A8Qv96Mf_7"},"source":["# Some misc test functions (ignore)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLY9gWNEmDIj"},"outputs":[],"source":["def alternative_evaluate(model): #change train to evaluate using this one to get best?? or of individual class idk\n","    model.eval()\n","\n","    with torch.no_grad():\n","        correct, total = 0, 0\n","\n","        for index, (data, target) in enumerate(tqdm(val_loader)): #iterate through val_loader in batches of inputs&targets in sizes of batch_size\n","            data, target = data.to(device), target.to(device)\n","\n","            outputs = model(data).cpu()\n","\n","            preds = np.array(outputs)\n","            target_labels = target.cpu().numpy()\n","            #preds and target_labels is an array of len batch_size with each row containing an array of len 4 of class pred/labels\n","\n","            #go through all the predictions, if greater than threshold set to 1, else 0\n","            #bit of a hack- [0,0,0,0] should never exist, convert to [0,0,0,1]\n","            # ['pneumonia', 'pneumothorax', 'pleural effusion', 'normal']\n","            for(i, pred) in enumerate(preds):\n","              preds[i] = (pred > threshold).astype(int)\n","              if(np.all(preds[i] == 0)):\n","                preds[i] = [0, 0, 0, 1]\n","\n","            for i in range(batch_size):\n","              if(i >= len(preds[i]) or i >= len(target_labels[i])):\n","                break\n","              if(np.array_equal(preds[i], target_labels[i])):\n","                correct += 1\n","              total += 1\n","        print(f\"Accuracy: {correct / total * 100:.2f}%\")\n","\n","def alternative_evaluate_2(model):\n","    model.eval()\n","\n","    with torch.no_grad():\n","        total_results = []\n","        total_targets = []\n","\n","        for index, (data, target) in enumerate(tqdm(val_loader)):\n","            data, target = data.to(device), target.to(device)\n","            outputs = model(data)\n","            total_results.extend(outputs.cpu().numpy())\n","            total_targets.extend(target.cpu().numpy())\n","\n","        #calculate_metrics_original(np.array(total_results), np.array(total_targets))\n","        calculate_metrics(np.array(total_results), np.array(total_targets))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37032,"status":"ok","timestamp":1719448524695,"user":{"displayName":"Cody Chen","userId":"04636862374912145551"},"user_tz":420},"id":"jd1lBNEQobNl","outputId":"2dd24eab-57cf-4156-9821-128c3620cbac"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 53/53 [00:36<00:00,  1.44it/s]"]},{"name":"stdout","output_type":"stream","text":["F1: [0.         0.52909418 0.         0.        ]\n","F1 Macro: 0.1322735452909418\n","AUC: [0.56815749 0.57825563 0.61725626 0.50773807]\n","AUC Macro: 0.5678518635904377\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["alternative_evaluate_2(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVLHAhIFq9_e"},"outputs":[],"source":["def unencode_multi_hot(encoded_labels, label_space):\n","    original_labels = []\n","    for label_vector in encoded_labels:\n","        labels = [label_space[idx] for idx, value in enumerate(label_vector) if value == 1]\n","        original_labels.append(labels)\n","    return original_labels\n","\n"]},{"cell_type":"markdown","metadata":{"id":"72lIXactl_T3"},"source":["# Evaluating on Test Dataset\n"]},{"cell_type":"markdown","metadata":{"id":"ulh92qnUzQOj"},"source":["# Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4676,"status":"ok","timestamp":1719458065746,"user":{"displayName":"Cody Chen","userId":"04636862374912145551"},"user_tz":420},"id":"zJpCLCDSzPle","outputId":"d5cc459a-940b-48f8-c7f1-ffcffbd18dde"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Resnext50(num_classes)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","checkpoint = torch.load(r'/content/drive/MyDrive/Cody - AIMI 2024/Trains/Run_2/best.ckpt')\n","model.load_state_dict(checkpoint['state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fs8xXHMIxc5T"},"outputs":[],"source":["test_transforms = v2.Compose([\n","    v2.ToImage(),\n","    v2.Resize(size=(224, 224), antialias=True),\n","    v2.ToDtype(torch.float32, scale=True),\n","    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","def getImage(image_path):\n","  image = Image.open(image_path).convert(\"RGB\")\n","  image = test_transforms(image)\n","  return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFCaq_rtwSM8"},"outputs":[],"source":["threshold = 0.5\n","\n","def predict(model, image_tensor):\n","  model.eval()\n","  with torch.no_grad():\n","      input = image_tensor.unsqueeze(0)  #image lacks batch layer, so insert a batch dimension of size 1\n","      input = input.to(device)\n","\n","      outputs = model(input).cpu()\n","      preds = np.array(outputs)\n","\n","      rounded_preds, thresholded_preds = np.empty_like(preds), np.empty_like(preds)\n","      rounded_preds[:] = preds\n","      thresholded_preds[:] = preds\n","\n","      for(i, pred) in enumerate(preds):\n","              thresholded_preds[i] = (pred > threshold).astype(int)\n","              if(np.all(thresholded_preds[i] == 0)):\n","                thresholded_preds[i] = [0, 0, 0, 1]\n","              rounded_preds[i] = [round(num, 7) for num in pred]\n","\n","      return thresholded_preds[0], rounded_preds[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tf96XcSktGJg"},"outputs":[],"source":["test_dataframe = pd.read_csv(\"/content/drive/MyDrive/Cody - AIMI 2024/2024 AIMI Summer Internship - Intern Materials/Datasets/test_annotations.csv\")\n","os.chdir(r'/content/student_data_split')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72284,"status":"ok","timestamp":1719458155477,"user":{"displayName":"Cody Chen","userId":"04636862374912145551"},"user_tz":420},"id":"jlt4y36Dt8fC","outputId":"ea32a312-4c2f-4c11-d793-0b69631fc213"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2983/2983 [01:11<00:00, 41.45it/s]\n"]}],"source":["#np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n","\n","processed_patients = []\n","#number_pneumonia = 0 #temporary just to double check\n","\n","for index, row in tqdm(test_dataframe.iterrows(), total=test_dataframe.shape[0]):\n","  images = os.listdir(row['study_id'])\n","  for image in images:\n","    thresholded_preds, rounded_preds = predict(model, getImage(row['study_id'] + '/' + image))\n","\n","    patient = {\n","        'study_id' : row['study_id'],\n","        'Pneumothorax' : thresholded_preds[1],\n","        'Pneumonia' : thresholded_preds[0],\n","        'Pleural Effusion' : thresholded_preds[2],\n","        'No Finding' : thresholded_preds[3],\n","        'Pneumothorax Probs' : rounded_preds[1],\n","        'Pneumonia Probs' : rounded_preds[0],\n","        'Pleural Effusion Probs' : rounded_preds[2],\n","        'No Finding Probs' : rounded_preds[3],\n","    }\n","\n","    #temporary, check # of pneumonia to make sure not exporting wrong\n","    #if(thresholded_preds[0] == 1):\n","    #  number_pneumonia += 1\n","\n","    processed_patients.append(patient)\n","\n","    break # too lazy to deal/combine output from multiple images for now, will handle later\n","\n","#print(f\"\\n {number_pneumonia} pneumonia detected\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rx7OPhYj9JHy"},"outputs":[],"source":["test_processed_dataframe = pd.DataFrame(processed_patients)\n","test_processed_dataframe.to_csv(r'/content/drive/MyDrive/Cody - AIMI 2024/test_results.csv', index=False, float_format='%.10f')"]},{"cell_type":"markdown","metadata":{"id":"l8F_1wzY6I7j"},"source":["## Submitting Your Results\n","Once you have successfully trained your model, generate predictions on the test set and save your results as a `.csv` file. This file can then be uploaded to the leaderboard: https://vilmedic.app/misc/aimi24/leaderboard.\n","\n","An example `test_results.csv` has been provided for reference only in the `2024 AIMI Summer Internship - Intern Materials/Datasets/Labels` folder. *Do not submit this, the results will be really poor. *\n","\n","Your final `.csv` file **must** have the following format:\n","- There must be a column titled `study_id` with the paths to the study_id for the test set image, e.g. `student_test/patient35172/study3`.\n","- The provided columns from `test_annotations.csv` must be present: \"Pneumothorax\", \"Pneumonia\", \"Pleural Effusion\", \"No Finding:\n","  - Each of these columns must contain a binary value `0` or `1` representing the **observed/ground-truth** absence or presence of the disease status.\n","- Added columns \"Pneumothorax Probs\", \"Pneumonia Probs\", \"Pleural Effusion Probs\", \"No Finding Probs\" containing the singular probability values belonging to each class.\n","  - Each of these columns must contain a continuous value representing the **predicted** probability of the absence or presence of the disease status for that class.\n","  - *Hint:* Depending on which loss function you used, you might already be outputing probabilities. You can then derive predictions by thresholding your probabilities to a binarized output. If your model outputs logits directly, then apply the sigmoid activation function `torch.sigmoid(logits)` to get probabilities and then threshold to get binary predictions.\n","- Double check that the length of the dataset passed into your dataloader matches the length of your final dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5osagFf5Fox"},"outputs":[],"source":["model = # Model Architecture\n","ckpt = torch.load(\"/content/best.pkl\")\n","model.load_state_dict(ckpt[\"state_dict\"])\n","\n","test_dataset = ChestXRayDataset(\"\"\"Fill in args here\"\"\")\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-y7sJCZEmgxg"},"outputs":[],"source":["# Write method to load in data from test_loader, compute model predictions, and append results to test_results dict\n","test_results = {\"image_path\": [], \"pred\": []}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-6CgEWNWHdl"},"outputs":[],"source":["test_results = pd.DataFrame(test_results)\n","test_results.to_csv(f\"/content/test_results.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuRWCSiVXL6n"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1LlGymoFHcd9IuEX7NT2Hv3sCLIzFeQpj","timestamp":1719252900880}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
